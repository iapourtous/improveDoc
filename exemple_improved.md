# Intelligence Artificielle

## Introduction

L'intelligence artificielle est un domaine informatique qui vise à créer des systèmes capables de résoudre des problèmes normalement réservés à l'intelligence humaine. Elle a été définie pour la première fois en 1956 lors de la [conférence de Dartmouth](https://fr.wikipedia.org/wiki/Projet_de_recherche_d%27%C3%A9t%C3%A9_de_Dartmouth_en_intelligence_artificielle) par [John McCarthy](https://fr.wikipedia.org/wiki/John_McCarthy), qui a proposé de « faire que les machines utilisent le langage, forment des concepts et résolvent des problèmes habituellement attribués à l’homme ».

Ce champ pluridisciplinaire regroupe plusieurs approches et techniques, parmi lesquelles :

- [l’apprentissage automatique](https://fr.wikipedia.org/wiki/Apprentissage_automatique), qui permet aux machines d’améliorer leurs performances à partir de données.  
- [l’apprentissage profond](https://fr.wikipedia.org/wiki/Apprentissage_profond), s’appuyant sur des réseaux de neurones artificiels à plusieurs couches pour traiter des problématiques complexes.  
- le traitement automatique du langage naturel, qui concerne la compréhension et la génération de textes ou de discours.  
- la vision par ordinateur, qui vise à analyser et comprendre le contenu d’images et de vidéos.  
- la robotique intelligente, où l’IA est intégrée dans des systèmes physiques pour interagir avec l’environnement.  

Ces systèmes sont aujourd’hui utilisés dans de nombreux domaines : assistants virtuels, recommandations personnalisées, diagnostic médical, véhicules autonomes, jeux et simulation. Ils explorent constamment de nouveaux défis, tels que l’explicabilité des décisions et les enjeux éthiques liés à leur déploiement.

## Histoire

L'IA a connu ses débuts dans les années 1950, avec les travaux d’[Alan Turing](https://fr.wikipedia.org/wiki/Alan_Turing) et d'autres pionniers qui ont posé les fondements théoriques de ce domaine. En 1950, Alan Turing publie son article « Computing Machinery and Intelligence », où il propose ce qui deviendra le [test de Turing](https://fr.wikipedia.org/wiki/Test_de_Turing) pour évaluer la capacité d’une machine à exhiber un comportement intelligent. Puis, en 1956, John McCarthy organise la [conférence de Dartmouth](https://fr.wikipedia.org/wiki/Conf%C3%A9rence_de_Dartmouth), où pour la première fois est définie officiellement l’intelligence artificielle comme discipline, avec l’objectif de « faire que les machines utilisent le langage, forment des concepts et résolvent des problèmes habituellement attribués à l’homme ».

Durant la fin des années 1950 et le début des années 1960, plusieurs programmes pionniers voient le jour :

- le Logic Theorist (1956) d’Allen Newell et Herbert A. Simon, souvent considéré comme le premier programme d’IA, capable de démontrer automatiquement des théorèmes mathématiques.  
- le [perceptron](https://fr.wikipedia.org/wiki/Perceptron) (1957) de Frank Rosenblatt, un modèle de réseau de neurones capable d’apprendre par renforcement simple.  
- le langage de programmation [Lisp](https://fr.wikipedia.org/wiki/Lisp_(langage)) (1958) inventé par John McCarthy, qui devient rapidement l’outil de référence pour la recherche en IA.  

Ces réussites initiales ouvrent la voie à une période d’optimisme (fin des années 1950–1960) marquée par des avancées en résolution de problèmes, en traitement automatique du langage et en représentation des connaissances. Cependant, les limitations technologiques et l’insuffisance des données conduisent dans les années 1970 à ce qu’on appellera le premier « hiver de l’IA », caractérisé par un financement réduit et des attentes déçues.

## Applications

Les applications de l’[IA](https://fr.wikipedia.org/wiki/Intelligence_artificielle) sont nombreuses et touchent de nombreux secteurs :

- **Médecine** : utilisation d’algorithmes d’[apprentissage automatique](https://fr.wikipedia.org/wiki/Apprentissage_automatique) pour l’analyse d’images médicales (détection de tumeurs en radiographie), la médecine personnalisée via l’interprétation de données génomiques et l’assistance virtuelle aux patients (infirmières virtuelles). Le marché mondial de l’IA en santé était estimé à 6,9 milliards USD en 2019, avec un taux de croissance annuel de 41 % jusqu’en 2025.  
- **Finance** : recours à l’IA pour le trading algorithmique, la gestion des risques, la détection des fraudes, le scoring de crédit et l’automatisation du service client (chatbots). On y trouve du traitement automatique du langage pour l’analyse de sentiments des actualités financières et des modèles de [deep learning](https://fr.wikipedia.org/wiki/Apprentissage_profond) pour la tarification des dérivés ; les robo-conseillers géraient plus de 1 000 milliards USD d’actifs en 2020.  
- **Transport** : développement de [véhicules autonomes](https://fr.wikipedia.org/wiki/V%C3%A9hicule_autonome) s’appuyant sur la vision par ordinateur, le lidar, le radar et la fusion de capteurs pour percevoir l’environnement, des modules de planification de trajectoire et des architectures de contrôle pour la conduite sans intervention humaine (Waymo, Tesla Autopilot, GM Cruise). Plusieurs millions de miles ont été parcourus en mode autonome.  
- **Industrie** : mise en œuvre de l’[Industrie 4.0](https://fr.wikipedia.org/wiki/Industrie_4.0) combinant systèmes cyber‑physiques, IoT et IA pour créer des « usines intelligentes ». Principales applications : maintenance prédictive via l’analyse de données de capteurs, inspection automatisée de la qualité par vision par ordinateur et optimisation en temps réel de la chaîne logistique, pour un impact économique estimé à 3,7 trillions USD ajoutés au PIB mondial d’ici 2025.  

## Défis

Malgré ses avancées, l’[IA](https://fr.wikipedia.org/wiki/Intelligence_artificielle) fait face à de nombreux défis :

- **Éthiques** : questions de responsabilité et de transparence dans les prises de décision automatisées, protection de la [vie privée](https://fr.wikipedia.org/wiki/Vie_priv%C3%A9e) et gestion de données sensibles (surveillance de masse, respect des droits fondamentaux), lutte contre les [biais algorithmiques](https://fr.wikipedia.org/wiki/Biais_algorithmique), et élaboration de cadres normatifs et réglementaires (lignes directrices de l’[OCDE](https://fr.wikipedia.org/wiki/Organisation_de_coop%C3%A9ration_et_de_d%C3%A9veloppement_%C3%A9conomiques), proposition de règlement européen sur l’IA) pour garantir justice, liberté et bien‑être.  
- **Techniques** : manque d’explicabilité des modèles de [deep learning](https://fr.wikipedia.org/wiki/Apprentissage_profond), trade‑off entre performance et interprétabilité (XAI), vulnérabilité aux attaques adversariales et aux perturbations subtiles, besoin de volumes massifs de données et de ressources de calcul (coûts énergétiques et empreinte carbone), difficulté à généraliser hors du domaine d’apprentissage et enjeux de robustesse, de standardisation et d’évaluation des méthodes.  
- **Sociaux** : risque de pertes d’emplois dans certains secteurs et transformation des compétences, creusement de la fracture numérique entre pays et populations, amplification des inégalités via le biais algorithmique (reconnaissance faciale, scoring de crédit), enjeux de confiance et d’acceptation par les utilisateurs, et défis liés à la gouvernance de l’IA dans l’espace public et privé.